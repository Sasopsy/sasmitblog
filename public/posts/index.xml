<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Sasmit&#39;s Blog</title>
    <link>https://example.org/posts/</link>
    <description>Recent content in Posts on Sasmit&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 18:47:49 +0530</lastBuildDate>
    <atom:link href="https://example.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Math of DDPMs</title>
      <link>https://example.org/posts/ddpm/</link>
      <pubDate>Wed, 24 Apr 2024 18:47:49 +0530</pubDate>
      <guid>https://example.org/posts/ddpm/</guid>
      <description>Introduction This blog post dives deep into the mathematics behind Denoising Diffusion Probabilistic Models, breaking down the objective function that make DDPMs work. I tried to &amp;ldquo;hand-wave&amp;rdquo; as little math as possible in this post and tried my best to cover all the steps in this derivation.
DDPMs Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain to slowly add random noise to data and learn to reverse the diffusion to get the original data sample back from the noise.</description>
    </item>
    <item>
      <title>Generative Adversarial Networks</title>
      <link>https://example.org/posts/gans/</link>
      <pubDate>Fri, 12 Apr 2024 02:49:45 +0530</pubDate>
      <guid>https://example.org/posts/gans/</guid>
      <description>Introduction The main idea: To train generators using really good classifiers.
Intuitive Explanation Imagine a counterfeiter tasked with creating fake paintings and a detective tasked with discerning fake from real paintings. Both of them are thrown in a game where they have to out compete each other. Through this, both of them will get have to get better in the methods they use to win the game.
Mathematical Explanation There are two models: A generative model $G$ that tries to capture the data distribution and a discriminative model $D$ that estimates the probability of a particular sample came from the training data.</description>
    </item>
    <item>
      <title>Word2vec</title>
      <link>https://example.org/posts/word2vec/</link>
      <pubDate>Thu, 11 Apr 2024 22:54:10 +0530</pubDate>
      <guid>https://example.org/posts/word2vec/</guid>
      <description>Word Embeddings Word embeddings is the idea of representing word in the form of vectors to have some notion of similarity and difference between them. Say in a finite dimensional vector space, we want the vectors representing &amp;ldquo;cat&amp;rdquo; and &amp;ldquo;feline&amp;rdquo; to be closer together than the vectors representing &amp;ldquo;human&amp;rdquo; and &amp;ldquo;dog&amp;rdquo;. We want to embed some sort of semantic meaning to the numbers consisting of these vectors.
One-Hot Encodings Our first thought would be to just represent each word as a one-hot vector.</description>
    </item>
    <item>
      <title>Variational Autoencoders</title>
      <link>https://example.org/posts/vae/</link>
      <pubDate>Wed, 10 Apr 2024 23:41:23 +0530</pubDate>
      <guid>https://example.org/posts/vae/</guid>
      <description>Introduction An auto-encoder learns an efficient representation of data. It &amp;ldquo;compresses&amp;rdquo; said data into lower dimensional vector and also learns to reconstruct it from said vector called latent vector.
In variational auto-encoders, apart from also learning to re-construct from the compressed vector, it also learns a mapping to an underlying &amp;ldquo;latent space&amp;rdquo;. This mapping can then help us to generate new variations of the data.
Problem Statement Fig 1. Directed Graphical Model $\mathbf{z}$ is a continuous random variable sampled from some prior distribution \(p_\theta(\mathbf{z})\).</description>
    </item>
  </channel>
</rss>
